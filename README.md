# Using-UV-Light-Game-Steering--Wheel-Control
Control a virtual car using hand gestures with OpenCV and Python.â€  â€œReal-time hand gesture detection for steering using HSV color tracking.â€

Real-time hand gesture-based UV steering control using OpenCV and Python.

ğŸ“¦ requirements.txt
numpy>=1.24.0
opencv-python>=4.8.0
imutils>=0.5.4


âš ï¸ directkeys.py should be included in the project folder manually. Itâ€™s used for simulating key presses in Windows.

README.md
# ğŸ® UV Steering Hand Gesture Control

![Python](https://img.shields.io/badge/Python-3.x-blue)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

Control a virtual car in real-time using hand gestures detected through your webcam.  
This project uses OpenCV and Python to detect a colored object and simulate LEFT (`A`) and RIGHT (`D`) key presses.

---

## âœ¨ Features
- ğŸ– Real-time hand gesture detection using HSV color tracking
- â¬…ï¸â¡ï¸ Control LEFT (`A`) and RIGHT (`D`) movements
- ğŸ‘ï¸ Visual guidance with rectangles and labels on the webcam feed
- âš¡ Adjustable color detection for different objects
- ğŸ¨ Dark-themed webcam overlay for clear control zones

---

## ğŸ› ï¸ Tech Stack
- Python 3.x
- OpenCV (Computer Vision)
- NumPy (Numerical computations)
- imutils (Image/video utilities)
- directkeys.py (Keyboard simulation for Windows)

---

## ğŸ“‚ Project Structure


â”œâ”€â”€ uv_steering.py # Main application code
â”œâ”€â”€ directkeys.py # Keyboard control module
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ demo_image.gif # Optional demo screenshot/GIF


---

## âš™ï¸ Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/UV-Steering-Hand-Gesture-Control.git
cd UV-Steering-Hand-Gesture-Control


Create a virtual environment and activate it:

# Mac/Linux
python -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate


Install dependencies:

pip install -r requirements.txt


Ensure directkeys.py is present in the folder.

â–¶ï¸ Usage

Run the main script:

python uv_steering.py


Use a colored object (matching HSV values in code) to control steering.

Move the object left or right in the top half of the webcam to simulate A or D key presses.

Press q to quit the application.

ğŸ¨ HSV Color Adjustment

Modify the following in uv_steering.py to match your object:

colourLower = np.array([125, 50, 50])
colourUpper = np.array([150, 255, 255])

ğŸ–¼ Demo

<!-- Replace with actual GIF/screenshot -->

ğŸ”§ Future Enhancements

Add support for multiple objects and gestures

Implement smoother steering using trajectory prediction

Cross-platform support (Linux/macOS)

Add custom UI overlay for feedback

ğŸ¤ Contributing

Contributions are welcome!

Fork this repository

Create your feature branch (git checkout -b feature-name)

Commit your changes (git commit -m "Add feature")

Push to the branch (git push origin feature-name)

Open a Pull Request

ğŸ“œ License

This project is licensed under the MIT License. See LICENSE
 for details.

ğŸ™Œ Acknowledgements

OpenCV for computer vision

NumPy for numerical operations

imutils for video utilities

directkeys.py for keyboard simulation
